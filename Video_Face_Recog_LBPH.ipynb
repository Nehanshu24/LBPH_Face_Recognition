{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "resident-membrane",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OpenCV module\n",
    "import cv2\n",
    "#os module for reading training data directories and paths\n",
    "import os\n",
    "#numpy to convert python lists to numpy arrays as it is needed by OpenCV face recognizers\n",
    "import numpy as np#import OpenCV module\n",
    "\n",
    "#matplotlib for display our images\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ambient-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "#there is no label 0 in our training data so subject name for index/label 0 is empty\n",
    "subjects = [\"\", \"Amaan\", \"Surya\",\"Nehanshu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "republican-diversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HAAR\n",
    "#function to detect face using OpenCV\n",
    "def detect_face(img):\n",
    "    #convert the test image to gray image as opencv face detector expects gray images\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #load OpenCV face detector, I am using LBP which is fast\n",
    "    #there is also a more accurate but slow Haar classifier\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "    #let's detect multiscale (some images may be closer to camera than others) images\n",
    "    #result is a list of faces\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "    \n",
    "    #if no faces are detected then return original img\n",
    "    if (len(faces) == 0):\n",
    "        return None, None\n",
    "    \n",
    "#     #under the assumption that there will be only one face,\n",
    "#     #extract the face area\n",
    "    (x, y, w, h) = faces[0]\n",
    "    \n",
    "    #return only the face part of the image\n",
    "    return gray[y:y+w, x:x+h],faces[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "boring-hampton",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function will read all persons' training images, detect face from each image\n",
    "#and will return two lists of exactly same size, one list \n",
    "# of faces and another list of labels for each face\n",
    "def prepare_training_data(data_folder_path):\n",
    "    \n",
    "    #------STEP-1--------\n",
    "    #get the directories (one directory for each subject) in data folder\n",
    "    dirs = os.listdir(data_folder_path)\n",
    "    \n",
    "    #list to hold all subject faces\n",
    "    faces = []\n",
    "    #list to hold labels for all subjects\n",
    "    labels = []\n",
    "    \n",
    "    #let's go through each directory and read images within it\n",
    "    for dir_name in dirs:\n",
    "        \n",
    "        #our subject directories start with letter 's' so\n",
    "        #ignore any non-relevant directories if any\n",
    "        if not dir_name.startswith(\"s\"):\n",
    "            continue;\n",
    "            \n",
    "        #------STEP-2--------\n",
    "        #extract label number of subject from dir_name\n",
    "        #format of dir name = slabel\n",
    "        #, so removing letter 's' from dir_name will give us label\n",
    "        label = int(dir_name.replace(\"s\", \"\"))\n",
    "        \n",
    "        #build path of directory containin images for current subject subject\n",
    "        #sample subject_dir_path = \"training-data/s1\"\n",
    "        subject_dir_path = data_folder_path + \"/\" + dir_name\n",
    "        \n",
    "        #get the images names that are inside the given subject directory\n",
    "        subject_images_names = os.listdir(subject_dir_path)\n",
    "        \n",
    "        #------STEP-3--------\n",
    "        #go through each image name, read image, \n",
    "        #detect face and add face to list of faces\n",
    "        for image_name in subject_images_names:\n",
    "            \n",
    "            #ignore system files like .DS_Store\n",
    "            if image_name.startswith(\".\"):\n",
    "                continue;\n",
    "            \n",
    "            #build image path\n",
    "            #sample image path = training-data/s1/1.pgm\n",
    "            image_path = subject_dir_path + \"/\" + image_name\n",
    "\n",
    "            #read image\n",
    "            image = cv2.imread(image_path)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #display an image window to show the image \n",
    "            cv2.imshow(\"Training on image...\", image)\n",
    "            cv2.waitKey(100)\n",
    "            \n",
    "            #detect face\n",
    "            face, rect = detect_face(image)\n",
    "            \n",
    "            #------STEP-4--------\n",
    "            #for the purpose of this tutorial\n",
    "            #we will ignore faces that are not detected\n",
    "            if face is not None:\n",
    "                #add face to list of faces\n",
    "                faces.append(face)\n",
    "                #add label for this face\n",
    "                labels.append(label)\n",
    "            \n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return faces, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "liable-grill",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Data prepared\n",
      "Total faces:  705\n",
      "Total labels:  705\n"
     ]
    }
   ],
   "source": [
    "#let's first prepare our training data\n",
    "#data will be in two lists of same size\n",
    "#one list will contain all the faces\n",
    "#and other list will contain respective labels for each face\n",
    "print(\"Preparing data...\")\n",
    "faces, labels = prepare_training_data(\"training-data\")\n",
    "print(\"Data prepared\")\n",
    "\n",
    "#print total faces and labels\n",
    "print(\"Total faces: \", len(faces))\n",
    "print(\"Total labels: \", len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "downtown-explorer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our LBPH face recognizer \n",
    "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "\n",
    "#or use EigenFaceRecognizer by replacing above line with \n",
    "#recognizer = cv2.face.EigenFaceRecognizer_create()\n",
    "\n",
    "#or use FisherFaceRecognizer by replacing above line with \n",
    "#recognizer = cv2.face.FisherFaceRecognizer_create()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "tribal-rugby",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train our face recognizer of our training faces\n",
    "recognizer.train(faces, np.array(labels))\n",
    "#recognizer.save('trainer300.yml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "illegal-milton",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to draw rectangle on image \n",
    "#according to given (x, y) coordinates and \n",
    "#given width and heigh\n",
    "def draw_rectangle(img, faces_d):\n",
    "    if len(faces_d) == 0:\n",
    "        faces_d = (0,0,0,0)\n",
    "    (x, y, w, h) = faces_d\n",
    "    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    \n",
    "#function to draw text on give image starting from\n",
    "#passed (x, y) coordinates. \n",
    "def draw_text(img, text, x, y):\n",
    "    cv2.putText(img, text, (x, y), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 255, 0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "corrected-bangkok",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face Detected:  [[219 190 249 249]]\n",
      "face Detected:  [[217 187 252 252]]\n",
      "face Detected:  [[216 185 251 251]]\n",
      "face Detected:  [[215 184 253 253]]\n",
      "face Detected:  [[216 181 256 256]]\n",
      "face Detected:  [[216 183 255 255]]\n",
      "face Detected:  [[218 183 241 241]]\n",
      "face Detected:  [[224 155 233 233]]\n",
      "face Detected:  [[235 151 225 225]]\n",
      "face Detected:  [[239 151 217 217]]\n",
      "face Detected:  [[235 150 229 229]]\n",
      "face Detected:  [[234 151 231 231]]\n",
      "face Detected:  [[233 160 222 222]]\n",
      "face Detected:  [[228 155 237 237]]\n",
      "face Detected:  [[224 162 237 237]]\n",
      "face Detected:  [[229 166 239 239]]\n",
      "face Detected:  [[224 160 247 247]]\n",
      "face Detected:  [[226 165 243 243]]\n",
      "face Detected:  [[225 167 237 237]]\n",
      "face Detected:  [[221 161 247 247]]\n",
      "face Detected:  [[225 164 238 238]]\n",
      "face Detected:  [[231 167 238 238]]\n",
      "face Detected:  [[233 162 246 246]]\n",
      "face Detected:  [[230 163 247 247]]\n",
      "face Detected:  [[235 175 234 234]]\n",
      "face Detected:  [[228 167 242 242]]\n",
      "face Detected:  [[235 172 236 236]]\n",
      "face Detected:  [[225 167 248 248]]\n",
      "face Detected:  [[228 171 248 248]]\n",
      "face Detected:  [[231 176 238 238]]\n",
      "face Detected:  [[228 167 252 252]]\n",
      "face Detected:  [[231 170 243 243]]\n",
      "face Detected:  [[231 168 245 245]]\n",
      "face Detected:  [[235 170 242 242]]\n",
      "face Detected:  [[231 168 242 242]]\n",
      "face Detected:  [[233 168 245 245]]\n",
      "face Detected:  [[231 165 245 245]]\n",
      "face Detected:  [[231 164 250 250]]\n",
      "face Detected:  [[235 169 241 241]]\n",
      "face Detected:  [[231 170 238 238]]\n",
      "face Detected:  [[230 170 245 245]]\n",
      "face Detected:  [[225 163 246 246]]\n",
      "face Detected:  [[227 164 246 246]]\n",
      "face Detected:  [[230 166 240 240]]\n",
      "face Detected:  [[225 165 245 245]]\n",
      "face Detected:  [[233 170 236 236]]\n",
      "face Detected:  [[227 168 244 244]]\n",
      "face Detected:  [[232 175 238 238]]\n",
      "face Detected:  [[233 177 239 239]]\n",
      "face Detected:  [[230 176 243 243]]\n",
      "face Detected:  [[228 176 246 246]]\n",
      "face Detected:  [[228 173 247 247]]\n"
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "# define a video capture object\n",
    "subjects = [\"\", \"Amaan\", \"Surya\",\"Nehanshu\"]\n",
    "face_classifier = cv2.CascadeClassifier(r\"C:\\Users\\nehan\\Documents\\Mega Sync\\VJTI\\Sum Pro\\Haar/cascades/haarcascade_frontalface_default.xml\")\n",
    "#recognizer.read(r\"C:\\Users\\nehan\\Documents\\Mega Sync\\VJTI\\Sum Pro\\LBPH\\trainer300.yml\")\n",
    "\n",
    "vid = cv2.VideoCapture(0)\n",
    "  \n",
    "while(True):\n",
    "      \n",
    "    # Capture the video frame\n",
    "    # by frame\n",
    "    ret, frame = vid.read()\n",
    "    #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # Predict part\n",
    "    im = frame.copy()\n",
    "    gray_im = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "    face_cascade = cv2.CascadeClassifier(r\"C:\\Users\\nehan\\Documents\\Mega Sync\\VJTI\\Sum Pro\\LBPH\\cascades\\haarcascade_frontalface_default.xml\")\n",
    "    #print (\"Confidence :\",confidence)\n",
    "    #print(\"label :\",label)\n",
    "    faces_d = face_cascade.detectMultiScale(gray_im, 1.1, 4)\n",
    "    print(\"face Detected: \",faces_d)\n",
    "    if len(faces_d) == 0:\n",
    "        faces_d = (0,0,0,0)\n",
    "    #roi_gray=gray_im[y:y+h,x:x+h]\n",
    "    for (x,y,w,h) in faces_d:\n",
    "        roi_gray=gray_im[y:y+h,x:x+h]\n",
    "        label,confidence=recognizer.predict(roi_gray)\n",
    "        cv2.rectangle(im,(x,y),(x+w,y+h),(0,255,0),thickness=5)\n",
    "        cv2.putText(im,subjects[label],(x,y),cv2.FONT_HERSHEY_DUPLEX,3,(255,0,0),6)\n",
    "    cv2.imshow('frame', im)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "  \n",
    "\n",
    "# After the loop release the cap object\n",
    "vid.release()\n",
    "# Destroy all the windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-spectacular",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
